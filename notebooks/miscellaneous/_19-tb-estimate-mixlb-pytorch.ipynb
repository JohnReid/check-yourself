{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "The purpose of this notebook is estimate the \"Mixed Logit B\" model of Brownstone and Train (1998) using pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare paths to where data is or should be stored\n",
    "DATA_PATH =\\\n",
    "    \"../../data/processed/model_ready_car_data.csv\"\n",
    "\n",
    "OUTPUT_PARAM_PATH =\\\n",
    "    \"../../models/estimated_mixlb_parameters.csv\"\n",
    "\n",
    "OUTPUT_GRADIENT_PATH =\\\n",
    "    \"../../models/estimated_mixlb_gradient.csv\"\n",
    "\n",
    "OUTPUT_HESSIAN_PATH =\\\n",
    "    \"../../models/estimated_mixlb_hessian.csv\"\n",
    "\n",
    "# Note needed column names\n",
    "ALT_ID_COLUMN = 'alt_id'\n",
    "OBS_ID_COLUMN = 'obs_id'\n",
    "CHOICE_COLUMN = 'choice'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "lines_to_next_cell": 0,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Built-in modules\n",
    "import sys\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "from typing import Callable\n",
    "\n",
    "# Third-party modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "import pylogit as pl\n",
    "import pylogit.mixed_logit_calcs as mlc\n",
    "\n",
    "# Local modules\n",
    "sys.path.insert(0, '../../')\n",
    "import src.models.mixlb as mixlb\n",
    "import src.models.torch_utils as utils\n",
    "from src.hessian import hessian\n",
    "from src.models.model_inputs import InputMixlB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load needed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_df = pd.read_csv(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the MIXL model and inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "mixl_model = mixlb.MIXLB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timothyb0912/minimamba/envs/checkYourself/lib/python3.7/site-packages/pylogit/choice_tools.py:703: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  design_matrix = np.hstack((x[:, None] for x in independent_vars))\n"
     ]
    }
   ],
   "source": [
    "# Create the various input objects needed for mixlb model.\n",
    "mixlb_input = InputMixlB.from_df(car_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target variables for the loss function\n",
    "torch_choices =\\\n",
    "    torch.from_numpy(car_df[CHOICE_COLUMN].values.astype(np.float32)).double()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the objective function\n",
    "Create the function to be used by `scipy.optimize.minimize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_scipy_closure(\n",
    "        input_obj: InputMixlB,\n",
    "        targets: torch.Tensor,\n",
    "        model: mixlb.MIXLB,\n",
    "        loss_func: Callable,\n",
    "        ) -> Callable:\n",
    "    \"\"\"\n",
    "    Creates the optimization function for use with scipy.optimize.minimize.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_obj : InputMixlB.\n",
    "        Container of the inputs for the model's probability function.\n",
    "    targets : 1D torch.Tensor\n",
    "        A Tensor of zeros and ones indicating which row was chosen for each\n",
    "        choice situation. Should have the same size as\n",
    "        `(input_obj.design.size()[0],)`.\n",
    "    model : MIXLB.\n",
    "        Should have a forward object that computes the probabilities of\n",
    "        the given discrete choice model.\n",
    "    loss_func : callable.\n",
    "        Should take as inputs, `model` outputs and `targets`. Should return\n",
    "        the value of the loss as well as the gradient of the loss.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    optimization_func : callable\n",
    "        Takes a set of parameters as a 1D numpy array and returns the\n",
    "        corresponding loss function value and gradient corresponding to the\n",
    "        passed parameters.\n",
    "    \"\"\"\n",
    "    def closure(params):\n",
    "        # params -> loss, grad\n",
    "        # Load the parameters onto the model\n",
    "        model.set_params_numpy(params)\n",
    "        # Ensure the gradients are summed starting from zero\n",
    "        model.zero_grad()\n",
    "        # Calculate the probabilities\n",
    "        probabilities =\\\n",
    "            model(design_2d=input_obj.design,\n",
    "                  rows_to_obs=input_obj.obs_mapping,\n",
    "                  rows_to_mixers=input_obj.mixing_mapping,\n",
    "                  normal_rvs_list=input_obj.normal_rvs)\n",
    "        # Calculate the loss\n",
    "        loss = loss_func(probabilities, targets)\n",
    "        # Compute the gradient.\n",
    "        loss.backward()\n",
    "        # Get the gradient.\n",
    "        grad = model.get_grad_numpy()\n",
    "        # Get a float version of the loss for scipy.\n",
    "        loss_val = loss.item()\n",
    "        return loss_val, grad\n",
    "    return closure\n",
    "\n",
    "scipy_objective =\\\n",
    "    make_scipy_closure(mixlb_input,\n",
    "                       torch_choices,\n",
    "                       mixl_model,\n",
    "                       utils.log_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate MIXLB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# Initialize parameters\n",
    "####\n",
    "# Initialize the model parameters to the final estimates from Brownstone & Train (1998),\n",
    "# taking care of the typo from the published paper.\n",
    "mean_array =\\\n",
    "    np.array([-1.5983748481622846, #-5.999,\n",
    "              -0.877,\n",
    "              -0.302,\n",
    "              -1.364,\n",
    "              -0.711,\n",
    "               1.541,\n",
    "              -1.748,\n",
    "               1.563,\n",
    "              -0.071,\n",
    "              -0.741,\n",
    "               0.897,\n",
    "               0.698,\n",
    "              -1.508,\n",
    "              -1.094,\n",
    "              -0.819,\n",
    "              -0.905,\n",
    "               0.359,\n",
    "               0.770,\n",
    "               0.621,\n",
    "               0.476,\n",
    "               0.335,\n",
    "               0,\n",
    "               0])\n",
    "\n",
    "std_dev_array =\\\n",
    "    np.array([6.808, 5.380, 2.289, 0.971])\n",
    "\n",
    "paper_estimates_array =\\\n",
    "    np.concatenate((mean_array, std_dev_array), axis=0)\n",
    "\n",
    "# Set the parameters on the model\n",
    "mixl_model.set_params_numpy(paper_estimates_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial MIXL: -7,370.09\n",
      "MNL:          -7,391.83\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "# Compute initial log-likelihood\n",
    "####\n",
    "with torch.no_grad():\n",
    "    # Compute the MIXL probabilities\n",
    "    initial_mixl_probs =\\\n",
    "        mixl_model.forward(design_2d=mixlb_input.design,\n",
    "                           rows_to_obs=mixlb_input.obs_mapping,\n",
    "                           rows_to_mixers=mixlb_input.mixing_mapping,\n",
    "                           normal_rvs_list=mixlb_input.normal_rvs)\n",
    "\n",
    "    # Compute the MIXL log-likelihood\n",
    "    initial_mixl_log_likelihood =\\\n",
    "        -1 * utils.log_loss(initial_mixl_probs, torch_choices)\n",
    "\n",
    "    # Compare the MIXL to MNL log-likelihoods\n",
    "    msg = 'Initial MIXL: {:,.2f}'\n",
    "    print(msg.format(initial_mixl_log_likelihood.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimation Time: 10.8 minutes\n"
     ]
    }
   ],
   "source": [
    "# Perform the optimization\n",
    "start_time = time.time()\n",
    "\n",
    "optimization_results =\\\n",
    "    minimize(scipy_objective,\n",
    "             paper_estimates_array,\n",
    "             jac=True,\n",
    "             method='bfgs')\n",
    "\n",
    "end_time = time.time()\n",
    "duration_sec = end_time - start_time\n",
    "duration_mins = duration_sec / 60.\n",
    "\n",
    "print('Estimation Time: {:.1f} minutes'.format(duration_mins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Log-likelihood: -7,370.09\n",
      "Final Log-Likelihood:    7,366.56\n"
     ]
    }
   ],
   "source": [
    "print('Initial Log-likelihood: {:,.2f}'.format(initial_mixl_log_likelihood))\n",
    "print('Final Log-Likelihood:    {:,.2f}'.format(optimization_results['fun']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.02063929e-10,  1.61201999e-07, -4.19390302e-08,  7.47850686e-08,\n",
       "       -1.82699654e-07,  3.43704994e-08, -1.54603583e-07, -5.11718366e-08,\n",
       "       -1.05335401e-08,  2.53901248e-07,  1.07174357e-08,  6.49821302e-09,\n",
       "        2.32708658e-08,  4.43429601e-08, -1.78816608e-08,  3.17923849e-08,\n",
       "        4.95778456e-08, -1.71724781e-08,  7.90312933e-09, -2.53907315e-07,\n",
       "       -2.36296594e-07,  0.00000000e+00,  0.00000000e+00, -2.60114483e-08,\n",
       "       -3.65452061e-08,  1.77963963e-08,  9.52168868e-08])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the gradient at the final parameters\n",
    "optimization_results['jac']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>initial</th>\n",
       "      <th>final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.598375</td>\n",
       "      <td>-1.511420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.877000</td>\n",
       "      <td>-0.741690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.302000</td>\n",
       "      <td>-0.291876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.364000</td>\n",
       "      <td>-1.472323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.711000</td>\n",
       "      <td>-0.655299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.541000</td>\n",
       "      <td>1.735805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.748000</td>\n",
       "      <td>-1.609506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.563000</td>\n",
       "      <td>1.505530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.071000</td>\n",
       "      <td>-0.027687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.741000</td>\n",
       "      <td>-0.539973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.897000</td>\n",
       "      <td>0.919872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.698000</td>\n",
       "      <td>0.703151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-1.508000</td>\n",
       "      <td>-1.510915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-1.094000</td>\n",
       "      <td>-1.104511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.819000</td>\n",
       "      <td>-0.816081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.905000</td>\n",
       "      <td>-0.873168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.359000</td>\n",
       "      <td>0.394959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.770000</td>\n",
       "      <td>0.830615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.621000</td>\n",
       "      <td>0.586968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.476000</td>\n",
       "      <td>0.614815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.335000</td>\n",
       "      <td>0.365396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>6.808000</td>\n",
       "      <td>8.423692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5.380000</td>\n",
       "      <td>5.093928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2.289000</td>\n",
       "      <td>2.524102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.971000</td>\n",
       "      <td>1.390528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     initial     final\n",
       "0  -1.598375 -1.511420\n",
       "1  -0.877000 -0.741690\n",
       "2  -0.302000 -0.291876\n",
       "3  -1.364000 -1.472323\n",
       "4  -0.711000 -0.655299\n",
       "5   1.541000  1.735805\n",
       "6  -1.748000 -1.609506\n",
       "7   1.563000  1.505530\n",
       "8  -0.071000 -0.027687\n",
       "9  -0.741000 -0.539973\n",
       "10  0.897000  0.919872\n",
       "11  0.698000  0.703151\n",
       "12 -1.508000 -1.510915\n",
       "13 -1.094000 -1.104511\n",
       "14 -0.819000 -0.816081\n",
       "15 -0.905000 -0.873168\n",
       "16  0.359000  0.394959\n",
       "17  0.770000  0.830615\n",
       "18  0.621000  0.586968\n",
       "19  0.476000  0.614815\n",
       "20  0.335000  0.365396\n",
       "21  0.000000  0.000000\n",
       "22  0.000000  0.000000\n",
       "23  6.808000  8.423692\n",
       "24  5.380000  5.093928\n",
       "25  2.289000  2.524102\n",
       "26  0.971000  1.390528"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare the final parameters to their starting values\n",
    "estimates_df =\\\n",
    "    pd.DataFrame({'initial': paper_estimates_array,\n",
    "                  'final': optimization_results['x']})\n",
    "estimates_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hessian Computation: 18.8 minutes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25, 25)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get rid of old gradient computations\n",
    "mixl_model.zero_grad()\n",
    "\n",
    "# Compute final probabilities\n",
    "final_mixl_probs =\\\n",
    "    mixl_model(design_2d=mixlb_input.design,\n",
    "               rows_to_obs=mixlb_input.obs_mapping,\n",
    "               rows_to_mixers=mixlb_input.mixing_mapping,\n",
    "               normal_rvs_list=mixlb_input.normal_rvs)\n",
    "\n",
    "# Compute final loss\n",
    "final_log_likelihood =\\\n",
    "    utils.log_loss(final_mixl_probs, torch_choices)\n",
    "\n",
    "# Compute the hessian of the loss\n",
    "hess_start_time = time.time()\n",
    "final_mixlb_hessian =\\\n",
    "    hessian(final_log_likelihood,\n",
    "            mixl_model.parameters())\n",
    "hess_end_time = time.time()\n",
    "hess_duration_sec = hess_end_time - hess_start_time\n",
    "hess_duration_mins = hess_duration_sec / 60.\n",
    "print('Hessian Computation: {:.1f} minutes'.format(hess_duration_mins))\n",
    "\n",
    "# Get the numpy array corresponding to the hessian\n",
    "final_mixlb_hessian_array = final_mixlb_hessian.numpy()\n",
    "\n",
    "# Extract the hessian that excludes the rows and columns\n",
    "# for the two constrained parameters\n",
    "desired_rows =\\\n",
    "    np.concatenate((np.arange(0, 21), np.arange(23, 27)), axis=0)\n",
    "final_mixlb_hessian_core =\\\n",
    "    final_mixlb_hessian_array[np.ix_(desired_rows, desired_rows)]\n",
    "final_mixlb_hessian_core.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final parameters\n",
    "estimates_df.final.to_csv(OUTPUT_PARAM_PATH,\n",
    "                          index=False,\n",
    "                          header=False)\n",
    "# Save the final gradient\n",
    "(pd.Series(optimization_results['jac'])\n",
    "   .to_csv(OUTPUT_GRADIENT_PATH, index=False))\n",
    "# Save the final hessian\n",
    "(pd.DataFrame(final_mixlb_hessian_array)\n",
    "   .to_csv(OUTPUT_HESSIAN_PATH, index=False, header=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Findings\n",
    "1. The most unexpected finding was that none of the parameter gradients was near zero when using the parameter values from the published article.\n",
    "\n",
    "2. When optimizing the model to get to a true local maximum of the log-likelihood function, there does not seem to be a huge difference in final results.\n",
    "The largest parameter change is the increase in the variance of non-EV utility functions.\n",
    "\n",
    "3. Computing the hessian of the estimated parameters takes a **very** long time.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
